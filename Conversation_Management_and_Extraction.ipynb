{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conversation Management & JSON Extraction with Groq API (OpenAI-compatible)\n",
        "\n",
        "This Colab-ready notebook implements:\n",
        "\n",
        "- Conversation management with history truncation and periodic summarization\n",
        "- JSON Schemaâ€“style classification & information extraction via OpenAI-style tool/function calling on the Groq API\n",
        "\n",
        "Resources:\n",
        "- [Groq API (OpenAI-compatible) docs](https://console.groq.com/docs)\n",
        "- [OpenAI Python SDK docs](https://platform.openai.com/docs/api-reference)\n",
        "\n",
        "Run all cells top-to-bottom. Provide a valid Groq API key when prompted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: install and imports\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import textwrap\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional, Callable\n",
        "\n",
        "# In Colab, ensure openai client is installed\n",
        "try:\n",
        "    import openai  # type: ignore\n",
        "except Exception:\n",
        "    !pip -q install --upgrade openai\n",
        "    import openai  # type: ignore\n",
        "\n",
        "# Configure OpenAI-compatible client for Groq\n",
        "# Groq exposes an OpenAI-compatible endpoint and key\n",
        "# Load from .env (no external libs) and allow environment override\n",
        "\n",
        "def load_env_file(path: str = \".env\") -> None:\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            return\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith(\"#\"):\n",
        "                    continue\n",
        "                if \"=\" in line:\n",
        "                    key, value = line.split(\"=\", 1)\n",
        "                    key = key.strip()\n",
        "                    value = value.strip().strip('\\'\"')\n",
        "                    if key and os.getenv(key) is None:\n",
        "                        os.environ[key] = value\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "load_env_file()\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\").strip()\n",
        "if not GROQ_API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        GROQ_API_KEY = getpass(\"Enter your GROQ_API_KEY: \")\n",
        "    except Exception:\n",
        "        GROQ_API_KEY = input(\"Enter your GROQ_API_KEY: \")\n",
        "\n",
        "# Keep env variable in sync for any downstream libraries\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "\n",
        "openai.api_key = GROQ_API_KEY\n",
        "openai.base_url = \"https://api.groq.com/openai/v1\"\n",
        "\n",
        "# Choose a Groq chat model compatible with OpenAI's Chat Completions\n",
        "# Recommended examples (subject to change):\n",
        "# - llama-3.1-70b-versatile\n",
        "# - llama-3.1-8b-instant\n",
        "# - mixtral-8x7b-32768\n",
        "GROQ_MODEL = os.getenv(\"GROQ_MODEL\") or \"llama-3.1-70b-versatile\"\n",
        "MODEL_FALLBACKS = [\n",
        "    \"llama-3.1-70b-versatile\",\n",
        "    \"llama-3.1-8b-instant\",\n",
        "    \"mixtral-8x7b-32768\",\n",
        "]\n",
        "print(\"Configured model:\", GROQ_MODEL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create OpenAI-compatible client for Groq (v1 SDK style)\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=GROQ_API_KEY)\n",
        "except Exception as e:\n",
        "    print(\"Falling back to legacy openai module usage due to:\", e)\n",
        "    client = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conversation Manager with truncation and periodic summarization\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "@dataclass\n",
        "class ConversationManager:\n",
        "    model: str\n",
        "    summarize_every_k: int = 3\n",
        "    system_prompt: str = \"You are a concise helpful assistant.\"\n",
        "    history: List[Message] = field(default_factory=list)\n",
        "    turn_counter: int = 0\n",
        "\n",
        "    def add_user(self, content: str) -> None:\n",
        "        self.history.append(Message(\"user\", content))\n",
        "\n",
        "    def add_assistant(self, content: str) -> None:\n",
        "        self.history.append(Message(\"assistant\", content))\n",
        "\n",
        "    def _messages_for_api(self) -> List[Dict[str, str]]:\n",
        "        msgs = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "        msgs.extend({\"role\": m.role, \"content\": m.content} for m in self.history)\n",
        "        return msgs\n",
        "\n",
        "    def chat(self, prompt: str) -> str:\n",
        "        self.add_user(prompt)\n",
        "        msgs = self._messages_for_api()\n",
        "        if client:\n",
        "            try:\n",
        "                resp = client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=msgs,\n",
        "                    temperature=0.2,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                # Try fallbacks if model is decommissioned or unavailable\n",
        "                fallback_used = None\n",
        "                for m in MODEL_FALLBACKS:\n",
        "                    try:\n",
        "                        resp = client.chat.completions.create(\n",
        "                            model=m,\n",
        "                            messages=msgs,\n",
        "                            temperature=0.2,\n",
        "                        )\n",
        "                        fallback_used = m\n",
        "                        break\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                if not fallback_used:\n",
        "                    raise e\n",
        "                else:\n",
        "                    self.model = fallback_used\n",
        "            output = resp.choices[0].message.content\n",
        "        else:\n",
        "            # Legacy fallback\n",
        "            try:\n",
        "                completion = openai.ChatCompletion.create(\n",
        "                    model=self.model,\n",
        "                    messages=msgs,\n",
        "                    temperature=0.2,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                fallback_used = None\n",
        "                for m in MODEL_FALLBACKS:\n",
        "                    try:\n",
        "                        completion = openai.ChatCompletion.create(\n",
        "                            model=m,\n",
        "                            messages=msgs,\n",
        "                            temperature=0.2,\n",
        "                        )\n",
        "                        fallback_used = m\n",
        "                        break\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                if not fallback_used:\n",
        "                    raise e\n",
        "                else:\n",
        "                    self.model = fallback_used\n",
        "            output = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "        self.add_assistant(output)\n",
        "        self.turn_counter += 1\n",
        "        if self.summarize_every_k > 0 and self.turn_counter % self.summarize_every_k == 0:\n",
        "            self.periodic_summarize_replace()\n",
        "        return output\n",
        "\n",
        "    def truncate_last_n_turns(self, n: int) -> None:\n",
        "        # One turn = user + assistant\n",
        "        if n <= 0:\n",
        "            self.history = []\n",
        "            return\n",
        "        to_keep = []\n",
        "        turns = 0\n",
        "        for m in reversed(self.history):\n",
        "            to_keep.append(m)\n",
        "            if m.role == \"assistant\":\n",
        "                turns += 1\n",
        "                if turns >= n:\n",
        "                    break\n",
        "        self.history = list(reversed(to_keep))\n",
        "\n",
        "    def truncate_by_char_limit(self, max_chars: int) -> None:\n",
        "        acc = 0\n",
        "        to_keep = []\n",
        "        for m in reversed(self.history):\n",
        "            if acc + len(m.content) <= max_chars:\n",
        "                to_keep.append(m)\n",
        "                acc += len(m.content)\n",
        "            else:\n",
        "                break\n",
        "        self.history = list(reversed(to_keep))\n",
        "\n",
        "    def truncate_by_word_limit(self, max_words: int) -> None:\n",
        "        acc = 0\n",
        "        to_keep = []\n",
        "        for m in reversed(self.history):\n",
        "            words = len(m.content.split())\n",
        "            if acc + words <= max_words:\n",
        "                to_keep.append(m)\n",
        "                acc += words\n",
        "            else:\n",
        "                break\n",
        "        self.history = list(reversed(to_keep))\n",
        "\n",
        "    def summarize_history(self, instruction: str = \"Summarize the conversation so far in 3-5 bullet points.\") -> str:\n",
        "        if not self.history:\n",
        "            return \"(empty)\"\n",
        "        msgs = self._messages_for_api() + [{\"role\": \"user\", \"content\": instruction}]\n",
        "        if client:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=msgs,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            summary = resp.choices[0].message.content\n",
        "        else:\n",
        "            completion = openai.ChatCompletion.create(\n",
        "                model=self.model,\n",
        "                messages=msgs,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            summary = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return summary\n",
        "\n",
        "    def periodic_summarize_replace(self) -> None:\n",
        "        summary = self.summarize_history(\n",
        "            \"Summarize the preceding conversation succinctly. Preserve key facts, intents, and constraints.\"\n",
        "        )\n",
        "        self.history = [Message(\"system\", self.system_prompt), Message(\"assistant\", f\"[Summary]\\n{summary}\")]\n",
        "\n",
        "cm = ConversationManager(model=GROQ_MODEL, summarize_every_k=3)\n",
        "cm.add_assistant(\"Hi! I can help with your questions.\")\n",
        "print(\"Initialized conversation manager with periodic summarization every 3 turns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: feed multiple conversation samples and show truncations\n",
        "\n",
        "samples = [\n",
        "    \"What's the weather like in Paris?\",\n",
        "    \"Can you also suggest 2 museums to visit?\",\n",
        "    \"Book me a table for two at 7pm near Louvre.\",\n",
        "    \"Change that to 8pm and make it for four people.\",\n",
        "]\n",
        "\n",
        "for i, s in enumerate(samples, 1):\n",
        "    print(f\"\\n--- Turn {i} ---\")\n",
        "    out = cm.chat(s)\n",
        "    print(\"Assistant:\", out[:200], (\"...\" if len(out) > 200 else \"\"))\n",
        "\n",
        "print(\"\\nHistory length after demo:\", len(cm.history))\n",
        "\n",
        "print(\"\\nTruncate to last 2 turns (user+assistant pairs):\")\n",
        "cm.truncate_last_n_turns(2)\n",
        "for m in cm.history:\n",
        "    print(m.role + \":\", m.content[:120])\n",
        "\n",
        "print(\"\\nTruncate by char limit (max 280 chars):\")\n",
        "cm.truncate_by_char_limit(280)\n",
        "for m in cm.history:\n",
        "    print(m.role + \":\", m.content[:120])\n",
        "\n",
        "print(\"\\nTruncate by word limit (max 60 words):\")\n",
        "cm.truncate_by_word_limit(60)\n",
        "for m in cm.history:\n",
        "    print(m.role + \":\", m.content[:120])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Periodic summarization demonstration\n",
        "\n",
        "cm2 = ConversationManager(model=GROQ_MODEL, summarize_every_k=3)\n",
        "cm2.add_assistant(\"Hello! I'm ready.\")\n",
        "seq = [\n",
        "    \"We discussed ordering pizza with extra cheese.\",\n",
        "    \"Then we considered switching to pasta instead.\",\n",
        "    \"Finally, confirm the pasta order for 2 people.\",\n",
        "    \"Add garlic bread please.\",\n",
        "]\n",
        "\n",
        "for i, s in enumerate(seq, 1):\n",
        "    print(f\"\\n== Step {i} ==\")\n",
        "    out = cm2.chat(s)\n",
        "    print(\"Assistant:\", out[:180], (\"...\" if len(out) > 180 else \"\"))\n",
        "    # Observe that after steps 3 and 6 etc., history will be replaced by summary\n",
        "    print(\"History snapshot (roles):\", [m.role for m in cm2.history])\n",
        "\n",
        "print(\"\\nFinal history after periodic summaries:\")\n",
        "for m in cm2.history:\n",
        "    print(m.role + \":\", m.content[:200])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: JSON Schema extraction via function calling\n",
        "\n",
        "# Define a JSON schema-ish descriptor and a matching tool/function\n",
        "extraction_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\"},\n",
        "        \"email\": {\"type\": \"string\"},\n",
        "        \"phone\": {\"type\": \"string\"},\n",
        "        \"location\": {\"type\": \"string\"},\n",
        "        \"age\": {\"type\": \"integer\"},\n",
        "    },\n",
        "    \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"],\n",
        "    \"additionalProperties\": False,\n",
        "}\n",
        "\n",
        "def get_tools_for_openai():\n",
        "    # OpenAI-style tool/function spec\n",
        "    return [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"extract_contact_info\",\n",
        "                \"description\": \"Extract structured contact details from a chat.\",\n",
        "                \"parameters\": extraction_schema,\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "def call_extraction(chat_text: str) -> Dict[str, Any]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You extract structured data. If fields are missing, infer conservatively or leave blank.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"From this chat, extract fields using the tool:\\n\\n{chat_text}\"},\n",
        "    ]\n",
        "\n",
        "    models_to_try = [GROQ_MODEL] + [m for m in MODEL_FALLBACKS if m != GROQ_MODEL]\n",
        "\n",
        "    if client:\n",
        "        # First pass: auto tool choice\n",
        "        last_err = None\n",
        "        resp = None\n",
        "        used_model = None\n",
        "        for m in models_to_try:\n",
        "            try:\n",
        "                resp = client.chat.completions.create(\n",
        "                    model=m,\n",
        "                    messages=messages,\n",
        "                    tools=get_tools_for_openai(),\n",
        "                    tool_choice=\"auto\",\n",
        "                    temperature=0.0,\n",
        "                )\n",
        "                used_model = m\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                continue\n",
        "        if resp is None:\n",
        "            raise last_err\n",
        "\n",
        "        choice = resp.choices[0]\n",
        "        tool_calls = choice.message.tool_calls or []\n",
        "        if tool_calls:\n",
        "            args = json.loads(tool_calls[0].function.arguments)\n",
        "            return args\n",
        "\n",
        "        # Second pass: force the tool call\n",
        "        resp2 = None\n",
        "        for m in models_to_try:\n",
        "            try:\n",
        "                resp2 = client.chat.completions.create(\n",
        "                    model=m,\n",
        "                    messages=messages,\n",
        "                    tools=get_tools_for_openai(),\n",
        "                    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_contact_info\"}},\n",
        "                    temperature=0.0,\n",
        "                )\n",
        "                used_model = m\n",
        "                break\n",
        "            except Exception:\n",
        "                continue\n",
        "        if resp2 is None:\n",
        "            return {}\n",
        "        choice2 = resp2.choices[0]\n",
        "        tool_calls2 = choice2.message.tool_calls or []\n",
        "        if tool_calls2:\n",
        "            return json.loads(tool_calls2[0].function.arguments)\n",
        "        return {}\n",
        "\n",
        "    else:\n",
        "        # Legacy path\n",
        "        last_err = None\n",
        "        completion = None\n",
        "        for m in models_to_try:\n",
        "            try:\n",
        "                completion = openai.ChatCompletion.create(\n",
        "                    model=m,\n",
        "                    messages=messages,\n",
        "                    tools=get_tools_for_openai(),\n",
        "                    tool_choice=\"auto\",\n",
        "                    temperature=0.0,\n",
        "                )\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                continue\n",
        "        if completion is None:\n",
        "            raise last_err\n",
        "\n",
        "        choice = completion[\"choices\"][0]\n",
        "        tool_calls = choice[\"message\"].get(\"tool_calls\") or []\n",
        "        if tool_calls:\n",
        "            args = json.loads(tool_calls[0][\"function\"][\"arguments\"])\n",
        "            return args\n",
        "\n",
        "        # Force tool\n",
        "        completion2 = None\n",
        "        for m in models_to_try:\n",
        "            try:\n",
        "                completion2 = openai.ChatCompletion.create(\n",
        "                    model=m,\n",
        "                    messages=messages,\n",
        "                    tools=get_tools_for_openai(),\n",
        "                    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_contact_info\"}},\n",
        "                    temperature=0.0,\n",
        "                )\n",
        "                break\n",
        "            except Exception:\n",
        "                continue\n",
        "        if completion2 is None:\n",
        "            return {}\n",
        "        choice2 = completion2[\"choices\"][0]\n",
        "        tool_calls2 = choice2[\"message\"].get(\"tool_calls\") or []\n",
        "        if tool_calls2:\n",
        "            return json.loads(tool_calls2[0][\"function\"][\"arguments\"])\n",
        "        return {}\n",
        "\n",
        "\n",
        "def validate_against_schema(data: Dict[str, Any], schema: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    errors = []\n",
        "    # Minimum manual validation without external libs\n",
        "    if schema.get(\"type\") == \"object\":\n",
        "        props = schema.get(\"properties\", {})\n",
        "        required = schema.get(\"required\", [])\n",
        "        for field in required:\n",
        "            if field not in data:\n",
        "                errors.append(f\"Missing required field: {field}\")\n",
        "        for k, v in data.items():\n",
        "            if k not in props and not schema.get(\"additionalProperties\", True):\n",
        "                errors.append(f\"Unexpected field: {k}\")\n",
        "        for k, prop in props.items():\n",
        "            if k in data:\n",
        "                expected_type = prop.get(\"type\")\n",
        "                val = data[k]\n",
        "                if expected_type == \"string\" and not (isinstance(val, str) or val is None):\n",
        "                    errors.append(f\"Field {k} should be string\")\n",
        "                if expected_type == \"integer\" and not (isinstance(val, int) or (isinstance(val, str) and val.isdigit())):\n",
        "                    errors.append(f\"Field {k} should be integer\")\n",
        "    return {\"ok\": len(errors) == 0, \"errors\": errors}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: parse at least 3 sample chats and validate\n",
        "\n",
        "sample_chats = [\n",
        "    \"\"\"\n",
        "    Hi I'm Alice Johnson. You can reach me at alice@example.com or +1-415-555-1212.\n",
        "    I'm currently in San Francisco, and I'm 29 years old.\n",
        "    \"\"\".strip(),\n",
        "    \"\"\"\n",
        "    This is Bob, email bob.smith@workmail.io, phone 020 7946 0958.\n",
        "    Based in London, age is 41.\n",
        "    \"\"\".strip(),\n",
        "    \"\"\"\n",
        "    Name: Carol Doe\n",
        "    Email: carol.d@example.org\n",
        "    Location: Toronto\n",
        "    Phone: (647) 555-9988\n",
        "    Age: 34\n",
        "    \"\"\".strip(),\n",
        "]\n",
        "\n",
        "extracted = []\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    print(f\"\\n### Chat {i}\")\n",
        "    print(chat)\n",
        "    data = call_extraction(chat)\n",
        "    print(\"Extracted:\", json.dumps(data, indent=2))\n",
        "    result = validate_against_schema(data, extraction_schema)\n",
        "    print(\"Validation:\", result)\n",
        "    extracted.append((data, result))\n",
        "\n",
        "print(\"\\nAll extractions complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- The conversation manager supports three truncation strategies and a configurable periodic summarization every k turns.\n",
        "- The extraction uses OpenAI-style tools on Groq's API. A lightweight manual validator checks the schema rules without external libraries.\n",
        "- Set `GROQ_API_KEY` and optionally `GROQ_MODEL` in the environment before running; otherwise you'll be prompted in Colab.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
